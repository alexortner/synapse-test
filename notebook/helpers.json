{
	"name": "helpers",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "Sparkpool01",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e34e1326-334e-4a0f-802b-803436acdccc"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/09a38f01-eb6d-4b29-8759-eaac6c6e0933/resourceGroups/lht_workshop/providers/Microsoft.Synapse/workspaces/lht-workshop-prepration/bigDataPools/Sparkpool01",
				"name": "Sparkpool01",
				"type": "Spark",
				"endpoint": "https://lht-workshop-prepration.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool01",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import datetime\n",
					"from datetime import datetime, timedelta\n",
					"import time\n",
					"import json\n",
					"import csv\n",
					"from delta import *\n",
					"from tabulate import tabulate\n",
					"STORAGE_BASE_PATH=bucket\n",
					"import pyspark.sql.functions as f\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"\n",
					"def list_parquet_files_recursive(path, visited_paths=None):\n",
					"    \"\"\" Recursively lists all .parquet files inside a given directory while avoiding infinite loops \"\"\"\n",
					"    if visited_paths is None:\n",
					"        visited_paths = set()\n",
					"\n",
					"    try:\n",
					"        # Ensure path is not revisited\n",
					"        if path in visited_paths:\n",
					"            return []\n",
					"        visited_paths.add(path)\n",
					"\n",
					"        # Check if path exists\n",
					"        if not mssparkutils.fs.exists(path):\n",
					"            print(f\"Skipping non-existent path: {path}\")\n",
					"            return []\n",
					"\n",
					"        # Get directory listing\n",
					"        items = mssparkutils.fs.ls(path)\n",
					"        if items is None:\n",
					"            print(f\"Warning: mssparkutils.fs.ls({path}) returned None\")\n",
					"            return []\n",
					"\n",
					"        parquet_files = []\n",
					"\n",
					"        for item in items:\n",
					"            if item.name.endswith(\".parquet\"):  # If it's a Parquet file, store it\n",
					"                parquet_files.append(item.path)\n",
					"            elif not item.name.startswith(\"_\") and \".\" not in item.name:  \n",
					"                # If it's a subdirectory, recurse into it\n",
					"                subdir_files = list_parquet_files_recursive(item.path, visited_paths)\n",
					"                parquet_files.extend(subdir_files)\n",
					"\n",
					"        return parquet_files\n",
					"\n",
					"    except Exception as e:\n",
					"        print(f\"Error accessing {path}: {e}\")\n",
					"        return []\n",
					"\n",
					"\n",
					"def ls(path, type=\"x\"):\n",
					"    try:\n",
					"        fsoutput = mssparkutils.fs.ls(STORAGE_BASE_PATH + path)\n",
					"\n",
					"        file_endings = [\".csv\", \".json\", \".avro\", \".parquet\", \"account=\", \"_delta_log\"]\n",
					"        file_endings_ignore = [\".crc\", \"__tmp\", \"checkpoint.parquet\"]\n",
					"\n",
					"        if type == \"p\":\n",
					"            # Call function on subfolder 'parquet'\n",
					"            parquet_files = list_parquet_files_recursive(STORAGE_BASE_PATH + path)\n",
					"\n",
					"            # Print cleaned output (without base path)\n",
					"            for file in parquet_files:\n",
					"                print(file.replace(STORAGE_BASE_PATH + \"/\", \"\"))\n",
					"\n",
					"        else:\n",
					"            for file in fsoutput:\n",
					"                if (any(x in file.name for x in file_endings)) and (not any(x in file.name for x in file_endings_ignore)):\n",
					"                    if \"/parquet\" in path:\n",
					"                        print(file.name.split(\"/\")[3])\n",
					"                        for subfile in mssparkutils.fs.ls(file.path):\n",
					"                            if \".parquet\" in subfile.name:\n",
					"                                print(subfile.path)\n",
					"                    else:\n",
					"                        print(file.name)\n",
					"    except Exception as e:\n",
					"        print(e)\n",
					"\n",
					"\n",
					"\n",
					"# Function to show content of one or several files with the same prefix/wildcard\n",
					"def cat(path, number_to_show=1):\n",
					"    '''Show content of one or several files with the same prefix or wildcard'''\n",
					"    try:\n",
					"        # List files in the specified path\n",
					"        fsoutput = mssparkutils.fs.ls(STORAGE_BASE_PATH + path)\n",
					"        filelist = []\n",
					"\n",
					"        # Define valid file types and ignored files\n",
					"        file_endings = [\".csv\", \".json\", \".avro\", \".parquet\", \"account=\"]\n",
					"        file_endings_ignore = [\".crc\", \"__tmp\", \"checkpoint.parquet\"]\n",
					"\n",
					"        # Filter valid files\n",
					"        for file in fsoutput:\n",
					"            if (any(x in file.name for x in file_endings)) and (not any(x in file.name for x in file_endings_ignore)):\n",
					"                if \"/parquet\" in path:\n",
					"                    for file2 in mssparkutils.fs.ls(file.path):\n",
					"                        if (\".parquet\" in file2.name) and (any(x in file2.name for x in file_endings)) and (not any(x in file2.name for x in file_endings_ignore)):\n",
					"                            filelist.append(file2.path)\n",
					"                else:\n",
					"                    filelist.append(file.path)\n",
					"\n",
					"        # Display content of the selected files\n",
					"        for file in filelist[:number_to_show]:\n",
					"            print(\"##########################################################\")\n",
					"            print(f\"File: {file.replace(STORAGE_BASE_PATH + '/', '')}\")\n",
					"            print(\"----------------------------------------------------------\")\n",
					"            print(mssparkutils.fs.head(file))  # Read and display file content\n",
					"\n",
					"    except Exception as e: \n",
					"        print(e)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"def show_table_details(db_name):\n",
					"    tables = spark.catalog.listTables(db_name)\n",
					"\n",
					"    results = []\n",
					"\n",
					"    for table in tables:\n",
					"        full_table_name = f\"{db_name}.{table.name}\"\n",
					"        \n",
					"        try:\n",
					"            details = spark.sql(f\"DESCRIBE EXTENDED {full_table_name}\").collect()\n",
					"            location_row = next((row for row in details if \"Location\" in row[0]), None)\n",
					"            location = location_row[1] if location_row else \"N/A\"\n",
					"            \n",
					"            results.append([table.name, table.tableType, location])\n",
					"        except Exception as e:\n",
					"            results.append([table.name, table.tableType, f\"Error: {e}\"])\n",
					"\n",
					"    # Print as table\n",
					"    print(tabulate(results, headers=[\"Table Name\", \"Type\", \"Path\"], tablefmt=\"github\"))\n",
					"\n",
					"\n",
					"def list_files_tree(path, indent=0, max_depth=1, ignore_folders=None):\n",
					"    if ignore_folders is None:\n",
					"        ignore_folders = [\"_delta_log\", \"__pycache__\",\".DS_Store\"]\n",
					"\n",
					"    try:\n",
					"        items = mssparkutils.fs.ls(path)\n",
					"        for item in items:\n",
					"            # Check if the current item's name matches any ignored folder\n",
					"            if any(ignored in item.name for ignored in ignore_folders):\n",
					"                continue\n",
					"\n",
					"            print(\"  \" * indent + f\"- {item.name}\")\n",
					"\n",
					"            # Only recurse into subdirectories if within depth and not ignored\n",
					"            if item.isDir and indent < max_depth:\n",
					"                list_files_tree(item.path, indent + 1, max_depth, ignore_folders)\n",
					"    except Exception as e:\n",
					"        print(\"  \" * indent + f\"[Error accessing {path}]: {e}\")"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# User Task test files\n",
					"from typing import Union\n",
					"from pyspark.sql.functions import input_file_name\n",
					"\n",
					"def check_5_files(user_count: Union[int, str], delta_path: str):\n",
					"    \"\"\"\n",
					"    Test if the manually observed parquet file count matches the actual number of Delta data files.\n",
					"    \n",
					"    Parameters:\n",
					"    - user_count: The number of files the user claims to have counted manually.\n",
					"    - delta_path: The Delta table path.\n",
					"    \n",
					"    Behavior:\n",
					"    - Filters for only .parquet files (excludes _delta_log)\n",
					"    - Handles cases where table was accidentally written multiple times (more files)\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Get actual list of files from mssparkutils (Synapse-friendly)\n",
					"        files = mssparkutils.fs.ls(delta_path)\n",
					"        parquet_files = [f for f in files if f.name.endswith(\".parquet\")]\n",
					"        actual_count = len(parquet_files)\n",
					"        \n",
					"        # Parse user input to int (in case they entered as string)\n",
					"        expected_count = int(user_count)\n",
					"\n",
					"        #print(f\"âœ… Actual number of .parquet data files in Delta table: {actual_count}\")\n",
					"        #print(f\"ðŸ‘¤ User-reported file count: {expected_count}\")\n",
					"\n",
					"        if actual_count == expected_count:\n",
					"            print(\"ðŸŽ‰ SUCCESS: Your file count is correct!\")\n",
					"        else:\n",
					"            print(\"âš ï¸ Oops! Your file count doesn't match.\")\n",
					"            print(f\"âœ… Actual number of .parquet data files in Delta table path {delta_path}: {actual_count}\")\n",
					"            print(f\"ðŸ‘¤ User-reported file count: {expected_count}\")\n",
					"            #print(\"Tip: Did you account for accidental overwrites or appends?\")\n",
					"    \n",
					"    except Exception as e:\n",
					"        print(f\"âŒ Error: {e}\")\n",
					"\n",
					"\n",
					"\n",
					"def check_5_database(user_database: str, expected_table: str = \"juices\"):\n",
					"    \"\"\"\n",
					"    Validates whether the given database contains the expected table.\n",
					"\n",
					"    Parameters:\n",
					"    - user_database: The database name the user identified (e.g., 'default').\n",
					"    - expected_table: The table name to look for (default: 'juices').\n",
					"    \"\"\"\n",
					"    try:\n",
					"        # Get all available databases\n",
					"        databases = [db.name for db in spark.catalog.listDatabases()]\n",
					"        if user_database not in databases:\n",
					"            print(f\"âš ï¸ Oops! Database '{user_database}' not found. Available databases: {databases}\")\n",
					"            return\n",
					"\n",
					"        # Use SQL to list tables in the given database\n",
					"        result = spark.sql(f\"SHOW TABLES IN {user_database}\").collect()\n",
					"        table_names = [row[\"tableName\"] for row in result]\n",
					"\n",
					"        if expected_table in table_names:\n",
					"            print(f\"ðŸŽ‰ SUCCESS: Table '{expected_table}' is correctly registered in database '{user_database}'\")\n",
					"        else:\n",
					"            print(f\"âš ï¸ Oops! Database '{user_database}' exists, but table '{expected_table}' was not found there.\")\n",
					"\n",
					"    except Exception as e:\n",
					"        print(f\"Error while checking table in database '{user_database}': {e}\")"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def safe_delete_path(path: str, recurse: bool = True):\n",
					"    \"\"\"\n",
					"    Deletes a path using mssparkutils.fs.rm(), but only prints output if there's an error.\n",
					"    Silently skips if the path does not exist.\n",
					"    \"\"\"\n",
					"    try:\n",
					"        mssparkutils.fs.rm(path, recurse=recurse)\n",
					"    except Exception as e:\n",
					"        if \"PathNotFoundException\" not in str(e):\n",
					"            print(f\"Error deleting path {path}: {e}\")"
				]
			}
		]
	}
}