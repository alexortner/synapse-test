{
	"name": "4_Demo_Parquet_Problem_Spark",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "Sparkpool01",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "34b5b569-1742-42e3-9a37-6ea6e7c9aec4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/09a38f01-eb6d-4b29-8759-eaac6c6e0933/resourceGroups/lht_workshop/providers/Microsoft.Synapse/workspaces/lht-workshop-prepration/bigDataPools/Sparkpool01",
				"name": "Sparkpool01",
				"type": "Spark",
				"endpoint": "https://lht-workshop-prepration.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool01",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pyspark.sql.functions as f\n",
					"bucket = \"abfss://lht@lhtdata.dfs.core.windows.net/\""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run helpers"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"list_files_tree(f\"{bucket}/airport.delta\",0,2)\n",
					"\n",
					"\n",
					"#spark.sql(\"DESCRIBE DETAIL Metastore_on_Files.airport\").show(truncate=False)\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"df_airports = (spark.read.table(\"Metastore_on_Files.airport\")\n",
					"              )\n",
					"\n",
					"(df_airports\n",
					"    .groupBy(f.col(\"country_code\"),f.col(\"country_name\"))\n",
					"    .count()\n",
					"    .sort(f.col(\"count\").desc())\n",
					").show(5,truncate=False)\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"+------------+------------------------+-----+\n",
					"|country_code|country_name            |count|\n",
					"+------------+------------------------+-----+\n",
					"|US          |United States of America|893  |\n",
					"|CA          |Canada                  |298  |\n",
					"|CN          |China                   |275  |\n",
					"|AU          |Australia               |217  |\n",
					"|RU          |Russian Federation      |215  |\n",
					"+------------+------------------------+-----+"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"(df_airports\n",
					"    .filter(f.col(\"country_code\")!=\"US\")\n",
					"    .write\n",
					"    .format(\"delta\")\n",
					"    .mode(\"overwrite\")\n",
					"    .saveAsTable(\"Metastore_on_Files.airport\")  \n",
					")\n",
					"\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# erzeuge ein Delta Tabellen Objekt\n",
					"deltaTable = DeltaTable.forName(spark,\"Metastore_on_Files.airport\")\n",
					"\n",
					"# Zeige die Versionshistory der DeltaTable (wird aus den Log Dateien erzeugt)\n",
					"deltaTable.history().select(\"version\",\"readVersion\",\"timestamp\",\"operation\",\"operationParameters.mode\",\"operationMetrics.numFiles\",\"operationMetrics.numOutputRows\").show(truncate=False)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"deltaTable = DeltaTable.forPath(spark, f\"{bucket}/airport.delta\")\n",
					"\n",
					"# Zeige die Versionshistory der DeltaTable (wird aus den Log Dateien erzeugt)\n",
					"deltaTable.history().select(\"version\",\"readVersion\",\"timestamp\",\"operation\",\"operationParameters.mode\",\"operationMetrics.numFiles\",\"operationMetrics.numOutputRows\").show(truncate=False)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"(df_airports\n",
					"    .filter(f.col(\"country_code\")==\"US\")\n",
					"    .write\n",
					"    .format(\"delta\")\n",
					"    .mode(\"overwrite\")\n",
					"    .save(f\"{bucket}/airport.delta\")  \n",
					")\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"list_files_tree(f\"{bucket}/airport.delta\",0,2)\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df_airports = (spark.read.table(\"Metastore_on_Files.airport\")\n",
					"              )\n",
					"\n",
					"(df_airports\n",
					"    .groupBy(f.col(\"country_code\"),f.col(\"country_name\"))\n",
					"    .count()\n",
					"    .sort(f.col(\"count\").desc())\n",
					").show(5,truncate=False)\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"df_airports = (spark.read.format(\"delta\").load(f\"{bucket}/airport.delta\")\n",
					"              )\n",
					"\n",
					"(df_airports\n",
					"    .groupBy(f.col(\"country_code\"),f.col(\"country_name\"))\n",
					"    .count()\n",
					"    .sort(f.col(\"count\").desc())\n",
					").show(5,truncate=False)"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Zusammenfassung  - Unbrauchbar für Deltatabellen\n",
					"- Synapse kann kein Delta\n",
					"- Von Delta Tabellen werden nur die Parquet Datein im Katalog registriert \n",
					"- Das führt zu Problemen bei der Verwendung der Delta Dateien in jeder Form von Tabellenabstraktion\n",
					"- **Deswegen kann in Synapse der Metastore kaum verwendet werden und es sollte immer mit Spark direkt auf Dateien gearbeitet werden**"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark.stop()"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					""
				]
			}
		]
	}
}